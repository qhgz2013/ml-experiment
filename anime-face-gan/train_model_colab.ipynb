{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nP_AK3iJGXgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Anime Face DCGAN\n",
        "Google Colaboratory GPU Edition\n",
        "\n",
        "## 1. Installing Google Drive API Package and authenticating"
      ]
    },
    {
      "metadata": {
        "id": "Y9imRkWM2uXt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQG9040bGqaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Defining some functions to upload or download files from Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "N3Bk1dMhm8Wb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def download_from_google_drive(file_in, path_out):\n",
        "  file_list = drive.ListFile({'q': 'trash=false'}).GetList()\n",
        "  file_id = ''\n",
        "  for file in file_list:\n",
        "    if file['title'] == file_in:\n",
        "      file_id = file['id']\n",
        "  if file_id == '':\n",
        "    raise ValueError('file not found')\n",
        "  downloaded = drive.CreateFile({'id': file_id})\n",
        "  downloaded.GetContentFile(path_out)\n",
        "  print('Download completed, %s (%s) -> %s' % (file_in, file_id, path_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YNeQOSUx3gEb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def upload_from_google_drive(file_out, path_in):\n",
        "  uploaded = drive.CreateFile({'title': file_out})\n",
        "  uploaded.SetContentFile(path_in)\n",
        "  uploaded.Upload()\n",
        "  print('Upload completed, %s -> %s (%s)' % (path_in, file_out, uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bg-QJPHPG-Ku",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Here is the code from [Anime Face GAN](https://github.com/qhgz2013/ml-experiment)\n",
        "The following code defines DCGAN model in tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "zoRo2VYW3sTg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q tqdm\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "#from getch import getch\n",
        "\n",
        "\n",
        "# a class to store variables in a dictionary\n",
        "class Vars(object):\n",
        "    def __init__(self):\n",
        "        self.d = dict()\n",
        "\n",
        "    def get(self, key):\n",
        "        return self.d[key]\n",
        "\n",
        "    def set(self, key, value):\n",
        "        self.d[key] = value\n",
        "\n",
        "    def __iter__(self):\n",
        "        for x in self.d:\n",
        "            yield x\n",
        "\n",
        "\n",
        "global_var = Vars()\n",
        "\n",
        "\n",
        "def load_training_set():\n",
        "    input_path = global_var.get('training_set_path')\n",
        "    cache_path = os.path.join(input_path, '../train.npy')\n",
        "    if os.path.exists(cache_path):\n",
        "        return np.load(cache_path)\n",
        "    files = os.listdir(input_path)\n",
        "    list_images = []\n",
        "    for file in files:\n",
        "        abs_path = os.path.join(input_path, file)\n",
        "        if os.path.isfile(abs_path):\n",
        "            image = io.imread(abs_path)\n",
        "            list_images.append(image)\n",
        "    ret_tensor = np.array(list_images)\n",
        "    # rescaling data\n",
        "    ret_tensor = (ret_tensor - 127.5) / 127.5\n",
        "    ret_tensor = ret_tensor.astype(np.float32)\n",
        "    np.save(cache_path, ret_tensor)\n",
        "    return ret_tensor\n",
        "\n",
        "\n",
        "def set_vram_growth(as_default_sess=True):\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    if as_default_sess:\n",
        "        return tf.InteractiveSession(config=config)\n",
        "    else:\n",
        "        return tf.Session(config=config)\n",
        "\n",
        "\n",
        "def rescale_to_rgb(image):\n",
        "    return (image + 1) / 2\n",
        "\n",
        "\n",
        "# tensorflow util functions\n",
        "def conv2d(i, output_dim, kernel_size=(5, 5), strides=(2, 2), stddev=0.02, name='conv2d'):\n",
        "    (k_h, k_w), (s_h, s_w) = kernel_size, strides\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable('w', [k_h, k_w, i.get_shape()[-1], output_dim],\n",
        "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        conv = tf.nn.conv2d(i, w, strides=[1, s_h, s_w, 1], padding='SAME')\n",
        "        b = tf.get_variable('b', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        conv = tf.nn.bias_add(conv, b)\n",
        "    return conv\n",
        "\n",
        "\n",
        "def deconv2d(i, output_shape, kernel_size=(5, 5), strides=(2, 2), stddev=0.02, name='deconv2d', output_weights=False):\n",
        "    (k_h, k_w), (s_h, s_w) = kernel_size, strides\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], i.get_shape()[-1]],\n",
        "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        if output_shape[0]:\n",
        "            deconv = tf.nn.conv2d_transpose(i, w, output_shape=output_shape, strides=[1, s_h, s_w, 1])\n",
        "        else:\n",
        "            deconv = tf.nn.conv2d_transpose(i, w, output_shape=[tf.shape(i)[0]] + output_shape[1:],\n",
        "                                            strides=[1, s_h, s_w, 1])\n",
        "            deconv = tf.reshape(deconv, [-1] + output_shape[1:])\n",
        "        b = tf.get_variable('b', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
        "        deconv = tf.nn.bias_add(deconv, b)\n",
        "    if output_weights:\n",
        "        return deconv, w, b\n",
        "    else:\n",
        "        return deconv\n",
        "\n",
        "\n",
        "def leaky_relu(x, alpha=0.2, name='leaky_relu'):\n",
        "    with tf.variable_scope(name):\n",
        "        return tf.maximum(x, alpha * x)\n",
        "\n",
        "\n",
        "def dense(i, output_dim, name='linear', stddev=0.02, output_weights=False):\n",
        "    shape = i.get_shape().as_list()\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable('w', [shape[1], output_dim], initializer=tf.random_normal_initializer(stddev=stddev))\n",
        "        b = tf.get_variable('b', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        mul = tf.matmul(i, w) + b\n",
        "    if output_weights:\n",
        "        return mul, w, b\n",
        "    else:\n",
        "        return mul\n",
        "\n",
        "\n",
        "def batch_norm(i, epsilon=1e-5, momentum=0.9, train=True, name='batch_norm'):\n",
        "    return tf.contrib.layers.batch_norm(i, decay=momentum, updates_collections=None, epsilon=epsilon, scale=True,\n",
        "                                        is_training=train, scope=name)\n",
        "\n",
        "\n",
        "def calc_conv_out_shape_same(size, stride):\n",
        "    return int(math.ceil(float(size) / float(stride)))\n",
        "\n",
        "\n",
        "def discriminator_model(input_tensor, dropout_tensor, reuse=False):\n",
        "    d_filter = global_var.get('d_filter')\n",
        "    d_arch = global_var.get('d_arch')\n",
        "    with tf.variable_scope('discriminator') as scope:\n",
        "        if reuse:\n",
        "            scope.reuse_variables()\n",
        "\n",
        "        # layer 1, not applying bn, uses leaky relu only\n",
        "        model = conv2d(input_tensor, d_filter, name='layer1/conv2d')\n",
        "        model = leaky_relu(model, name='layer1/lrelu')\n",
        "\n",
        "        if d_arch == 'selu':\n",
        "            def forward(inputs, name):\n",
        "                inputs = tf.nn.selu(inputs, name=name + '/selu')\n",
        "                inputs = tf.nn.dropout(inputs, dropout_tensor, name=name + '/dropout')\n",
        "                return inputs\n",
        "        elif d_arch == 'bn_first':\n",
        "            def forward(inputs, name):\n",
        "                inputs = batch_norm(inputs, name=name + '/bn')\n",
        "                inputs = leaky_relu(inputs, name=name + '/lrelu')\n",
        "                inputs = tf.nn.dropout(inputs, dropout_tensor, name=name + '/dropout')\n",
        "                return inputs\n",
        "        elif d_arch == 'bn_last':\n",
        "            def forward(inputs, name):\n",
        "                inputs = leaky_relu(inputs, name=name + '/lrelu')\n",
        "                inputs = tf.nn.dropout(inputs, dropout_tensor, name=name + '/dropout')\n",
        "                inputs = batch_norm(inputs, name=name + '/bn')\n",
        "                return inputs\n",
        "        else:\n",
        "            raise ValueError('Incorrect d_arch')\n",
        "\n",
        "        # layer 2 to 4\n",
        "        model = conv2d(model, d_filter * 2, name='layer2/conv2d')\n",
        "        model = forward(model, name='layer2')\n",
        "\n",
        "        model = conv2d(model, d_filter * 4, name='layer3/conv2d')\n",
        "        model = forward(model, name='layer3')\n",
        "\n",
        "        model = conv2d(model, d_filter * 8, name='layer4/conv2d')\n",
        "        model = forward(model, name='layer4')\n",
        "\n",
        "        model = tf.reshape(model, [tf.shape(model)[0], np.prod(model.get_shape().as_list()[1:])],\n",
        "                           name='layer5/flatten')\n",
        "        model_logits = dense(model, 1, name='layer5/dense')\n",
        "        model = tf.nn.sigmoid(model_logits, name='layer5/sigmoid')\n",
        "\n",
        "        return model, model_logits\n",
        "\n",
        "\n",
        "def generator_model(input_tensor, dropout_tensor, reuse=False, train=True):\n",
        "    image_width = global_var.get('image_width')\n",
        "    image_height = global_var.get('image_height')\n",
        "    g_filter = global_var.get('g_filter')\n",
        "    channel_count = global_var.get('channel_count')\n",
        "    g_arch = global_var.get('g_arch')\n",
        "    with tf.variable_scope('generator') as scope:\n",
        "        if reuse:\n",
        "            scope.reuse_variables()\n",
        "\n",
        "        if g_arch == 'selu':\n",
        "            def forward(inputs,  name):\n",
        "                inputs = tf.nn.selu(inputs, name=name + '/selu')\n",
        "                inputs = tf.nn.dropout(inputs, dropout_tensor, name=name + '/dropout')\n",
        "                return inputs\n",
        "        elif g_arch == 'bn_first':\n",
        "            def forward(inputs, name):\n",
        "                inputs = batch_norm(inputs, train=train, name=name + '/bn')\n",
        "                inputs = tf.nn.relu(inputs, name=name + '/relu')\n",
        "                inputs = tf.nn.dropout(inputs, dropout_tensor, name=name + '/dropout')\n",
        "                return inputs\n",
        "        elif g_arch == 'bn_last':\n",
        "            def forward(inputs, name):\n",
        "                inputs = tf.nn.relu(inputs, name=name + '/relu')\n",
        "                inputs = tf.nn.dropout(inputs, dropout_tensor, name=name + '/dropout')\n",
        "                inputs = batch_norm(inputs, train=train, name=name + '/bn')\n",
        "                return inputs\n",
        "        else:\n",
        "            raise ValueError('Incorrect g_arch')\n",
        "\n",
        "        # fc layer\n",
        "        size_h = calc_conv_out_shape_same(image_height, 16)\n",
        "        size_w = calc_conv_out_shape_same(image_width, 16)\n",
        "        model = dense(input_tensor, size_h * size_w * g_filter * 8, name='layer0/fc')\n",
        "        model = tf.reshape(model, [-1, size_h, size_w, g_filter * 8], name='layer0/reshape')\n",
        "        model = forward(model, name='layer0')\n",
        "\n",
        "        # deconv1\n",
        "        size_h = calc_conv_out_shape_same(image_height, 8)\n",
        "        size_w = calc_conv_out_shape_same(image_width, 8)\n",
        "        model = deconv2d(model, [None, size_h, size_w, g_filter * 4], name='layer1/deconv2d')\n",
        "        model = forward(model, name='layer1')\n",
        "\n",
        "        # deconv2\n",
        "        size_h = calc_conv_out_shape_same(image_height, 4)\n",
        "        size_w = calc_conv_out_shape_same(image_width, 4)\n",
        "        model = deconv2d(model, [None, size_h, size_w, g_filter * 2], name='layer2/deconv2d')\n",
        "        model = forward(model, name='layer2')\n",
        "\n",
        "        # deconv3\n",
        "        size_h = calc_conv_out_shape_same(image_height, 2)\n",
        "        size_w = calc_conv_out_shape_same(image_width, 2)\n",
        "        model = deconv2d(model, [None, size_h, size_w, g_filter], name='layer3/deconv2d')\n",
        "        model = forward(model, name='layer3')\n",
        "\n",
        "        # deconv4(output layer)\n",
        "        model = deconv2d(model, [None, image_height, image_width, channel_count], name='layer4/deconv2d')\n",
        "        model = tf.nn.tanh(model, name='layer4/tanh')\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "def compile_loss(d_logits, gan_logits, summary_dict):\n",
        "    d_loss_true = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\n",
        "    d_loss_fake = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits, labels=tf.zeros_like(gan_logits)))\n",
        "    d_loss = d_loss_true + d_loss_fake\n",
        "\n",
        "    summary_dict['d_loss'] = tf.summary.scalar('d_loss', d_loss)  # test summary\n",
        "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits, labels=tf.ones_like(gan_logits)))\n",
        "\n",
        "    summary_dict['g_loss'] = tf.summary.scalar('g_loss', g_loss)\n",
        "    return d_loss, g_loss\n",
        "\n",
        "\n",
        "prev_step = 0\n",
        "\n",
        "\n",
        "# this code is for saving the current weights of discriminator, generator model\n",
        "def save_model(sess, saver, fname, step):\n",
        "    saver.save(sess, fname + '/gan', global_step=step)\n",
        "    print('\\nmodel saved, save step %d' % step)\n",
        "    return saver\n",
        "\n",
        "\n",
        "# this code is for loading the saved weights of discriminator, generator model\n",
        "def load_model(sess, saver, fname):\n",
        "    ckpt = tf.train.get_checkpoint_state(fname)\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "        global prev_step\n",
        "        prev_step = int(re.search(re.compile('\\\\d+$'), ckpt.model_checkpoint_path)[0])\n",
        "    print('\\nmodel loaded, restore step %d' % prev_step)\n",
        "    return saver\n",
        "\n",
        "\n",
        "def save_images(output_name, images):\n",
        "    m, h, w, c = images.shape\n",
        "    rows = int(math.ceil(math.sqrt(m)))\n",
        "    cols = rows\n",
        "    out_image = np.zeros((rows * h, cols * w, c))\n",
        "    for y in range(rows):\n",
        "        for x in range(cols):\n",
        "            offset = y * cols + x\n",
        "            if offset >= m:\n",
        "                continue\n",
        "            out_image[y*h:(y+1)*h, x*w:(x+1)*w, :] = images[offset]\n",
        "    io.imsave(output_name, out_image)\n",
        "\n",
        "\n",
        "def train_model(sess, saver, train_x,\n",
        "                d_opt, g_opt, sampler,\n",
        "                image_input, noise_input, sample_noise, dropout_input,\n",
        "                summary_dict, summary_writer):\n",
        "    # retrieving the global variables\n",
        "    m = train_x.shape[0]\n",
        "    batch_size = global_var.get('batch_size')\n",
        "    noise_dim = global_var.get('noise_dim')\n",
        "    dropout_rate = global_var.get('dropout_rate')\n",
        "    test_generator_per_step = global_var.get('test_generator_per_step')\n",
        "    save_weights_per_step = global_var.get('save_weights_per_step')\n",
        "    random_count = global_var.get('random_count')\n",
        "    d_opt_runs_per_step = global_var.get('d_opt_runs_per_step')\n",
        "    g_opt_runs_per_step = global_var.get('g_opt_runs_per_step')\n",
        "    epochs = global_var.get('epochs')\n",
        "\n",
        "    # updating the step counter\n",
        "    global prev_step\n",
        "    step_sum = prev_step\n",
        "    # retrieving the summary variables\n",
        "    d_loss_sum = summary_dict['d_loss']\n",
        "    g_loss_sum = summary_dict['g_loss']\n",
        "\n",
        "    d_pred = summary_dict['d_pred']\n",
        "    g_pred = summary_dict['g_pred']\n",
        "    g_trace = summary_dict['g_trace']\n",
        "    d_lr_sum = summary_dict['d_lr']\n",
        "    d_beta1_sum = summary_dict['d_beta1']\n",
        "    g_lr_sum = summary_dict['g_lr']\n",
        "    g_beta1_sum = summary_dict['g_beta1']\n",
        "    # the epoch counter (from 0 every time)\n",
        "    i = 0\n",
        "    # the counter for d_opt runs\n",
        "    d_opt_has_ran = 0\n",
        "    \n",
        "    while True:\n",
        "        i += 1\n",
        "        if epochs <= 0:\n",
        "            #if getch() == 'q':\n",
        "            #    break\n",
        "            print('\\n[Epoch %d]' % i)\n",
        "        else:\n",
        "            if i > epochs:\n",
        "                break\n",
        "            print('\\n[Epoch %d of %d]' % (i, epochs))\n",
        "\n",
        "        # randomize the indices for the training set\n",
        "        random_idx = np.arange(m)\n",
        "        np.random.shuffle(random_idx)\n",
        "        # calculating how many steps should be run for one epoch\n",
        "        steps = int(math.ceil(m / batch_size))\n",
        "        for step in tqdm(range(steps), ascii=True):\n",
        "            # summarize the learning rate\n",
        "            summary, summary2 = sess.run([d_lr_sum, d_beta1_sum])\n",
        "            summary_writer.add_summary(summary, step_sum)\n",
        "            summary_writer.add_summary(summary2, step_sum)\n",
        "            summary, summary2 = sess.run([g_lr_sum, g_beta1_sum])\n",
        "            summary_writer.add_summary(summary, step_sum)\n",
        "            summary_writer.add_summary(summary2, step_sum)\n",
        "\n",
        "            # the indices of training set for current step\n",
        "            step_idx = random_idx[step * batch_size: (step + 1) * batch_size]\n",
        "            # the sample length of current step\n",
        "            length = len(step_idx)\n",
        "            images_real = train_x[step_idx]\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[length, noise_dim])\n",
        "\n",
        "            # training the discriminator\n",
        "            _, summary, summary2 = sess.run([d_opt, d_loss_sum, d_pred], feed_dict={image_input: images_real,\n",
        "                                                                                    noise_input: noise,\n",
        "                                                                                    dropout_input: 1.0 - dropout_rate})\n",
        "            summary_writer.add_summary(summary, step_sum)\n",
        "            summary_writer.add_summary(summary2, step_sum)\n",
        "            d_opt_has_ran += 1\n",
        "            # continue to train the discriminator if d_opt_runs_per_step > 1 (skips training the generator)\n",
        "            if d_opt_has_ran < d_opt_runs_per_step:\n",
        "                continue\n",
        "            d_opt_has_ran %= d_opt_runs_per_step\n",
        "\n",
        "            for _ in range(g_opt_runs_per_step):\n",
        "                _, summary = sess.run([g_opt, g_loss_sum], feed_dict={noise_input: noise,\n",
        "                                                                      dropout_input: 1.0 - dropout_rate})\n",
        "                summary_writer.add_summary(summary, step_sum)\n",
        "\n",
        "            # testing generator\n",
        "            if (step_sum + 1) % test_generator_per_step == 0:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[random_count, noise_dim])\n",
        "                img_pred, summary = sess.run([sampler, g_pred], feed_dict={noise_input: noise, dropout_input: 1.0})\n",
        "                summary_writer.add_summary(summary, step_sum + 1)\n",
        "                img_trace, summary = sess.run([sampler, g_trace], feed_dict={noise_input: sample_noise,\n",
        "                                                                             dropout_input: 1.0})\n",
        "                summary_writer.add_summary(summary, step_sum + 1)\n",
        "                \n",
        "                save_images(global_var.get('output_dir') + '/pred_%d_steps.png' % (step_sum + 1),\n",
        "                            rescale_to_rgb(img_pred))\n",
        "                save_images(global_var.get('output_dir') + '/trace_%d_steps.png' % (step_sum + 1),\n",
        "                            rescale_to_rgb(img_trace))\n",
        "            # saving weights\n",
        "            if (step_sum + 1) % save_weights_per_step == 0:\n",
        "                saver = save_model(sess, saver, global_var.get('weight_dir'), step_sum + 1)\n",
        "\n",
        "            step_sum += 1\n",
        "            # end step for\n",
        "        # end epoch for\n",
        "    # save model after exiting training process\n",
        "    save_model(sess, saver, global_var.get('weight_dir'), step_sum)\n",
        "    prev_step = step_sum\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-whdH0ppHltL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. The Pre-Training Process\n",
        "Defining all the training variables here, run it only once"
      ]
    },
    {
      "metadata": {
        "id": "jJpGBwWB3xlL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "abd4414e-e119-404a-8590-13ead8239bfc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521558054375,
          "user_tz": -480,
          "elapsed": 4652,
          "user": {
            "displayName": "qhgz2011@hotmail.com",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111469927240443201404"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# defining the output image shape for the generator\n",
        "image_width = 100\n",
        "image_height = 100\n",
        "# defining the dimension of noise vector for the generator (also called z_dim)\n",
        "noise_dim = 100\n",
        "# defining the minimal filter size for g(generator) and d(discriminator)\n",
        "# for the DCGAN paper, the filter size is [1024, 512, 256, 128] (except the last value 3), so set this value to 128\n",
        "g_filter = 64\n",
        "d_filter = 64\n",
        "# defining the batch size for batch training\n",
        "batch_size = 128\n",
        "# defining the file path for loading the training set\n",
        "training_set_path = 'train'\n",
        "# defining the image color channel count (for default only, will be overwritten after loading the training set)\n",
        "channel_count = 3\n",
        "# defining the learning rate and momentum value for Adam optimizer\n",
        "d_adam_lr = 0.0005\n",
        "d_adam_beta1 = 0.5\n",
        "g_adam_lr = 0.0002\n",
        "g_adam_beta1 = 0.5\n",
        "# defining the number used to generate the image during the training process\n",
        "# `random_count` is used to generate images with random noise\n",
        "# `sample_count` is used to generate images with fixed noise\n",
        "random_count = 64\n",
        "sample_count = 64\n",
        "# defining the dropout rate for the learning process, set it to 0.0 to disable dropout layer\n",
        "dropout_rate = 0.0\n",
        "# defining how many training steps the generator should be tested\n",
        "test_generator_per_step = 100\n",
        "# defining how many training steps the weights should be saved\n",
        "save_weights_per_step = 500\n",
        "# defining how many times discriminator and generator optimizer should run in one training step\n",
        "d_opt_runs_per_step = 1\n",
        "g_opt_runs_per_step = 1\n",
        "# defining the directory for storing model weights, the logs and generator outputs\n",
        "weight_dir = 'model.run2'\n",
        "log_dir = 'log.run2'\n",
        "output_dir = 'output.run2'\n",
        "# defining the architecture of D and G\n",
        "# `selu` uses the SeLU activation function (Self-normalized Linear Unit), only followed by a dropout layer\n",
        "# `bn_first` uses ReLU activation for G and LeakyReLU for D, ordered by (De)Conv2D -> BN -> Activation -> Dropout\n",
        "# `bn_last` uses ReLU activation for G and LeakyReLU for G, ordered by (De)Conv2D -> Activation -> Dropout -> BN\n",
        "d_arch = 'bn_first'\n",
        "g_arch = 'bn_first'\n",
        "# epochs for training, set it to -1 if you want to exit the program by pressing `Q`\n",
        "epochs = 100\n",
        "\n",
        "# save to global variables\n",
        "global_var.set('image_width', image_width)\n",
        "global_var.set('image_height', image_height)\n",
        "global_var.set('noise_dim', noise_dim)\n",
        "global_var.set('g_filter', g_filter)\n",
        "global_var.set('d_filter', d_filter)\n",
        "global_var.set('batch_size', batch_size)\n",
        "global_var.set('training_set_path', training_set_path)\n",
        "global_var.set('channel_count', channel_count)\n",
        "global_var.set('d_adam_lr', d_adam_lr)\n",
        "global_var.set('d_adam_beta1', d_adam_beta1)\n",
        "global_var.set('g_adam_lr', g_adam_lr)\n",
        "global_var.set('g_adam_beta1', g_adam_beta1)\n",
        "global_var.set('random_count', random_count)\n",
        "global_var.set('sample_count', sample_count)\n",
        "global_var.set('dropout_rate', dropout_rate)\n",
        "global_var.set('test_generator_per_step', test_generator_per_step)\n",
        "global_var.set('save_weights_per_step', save_weights_per_step)\n",
        "global_var.set('d_opt_runs_per_step', d_opt_runs_per_step)\n",
        "global_var.set('g_opt_runs_per_step', g_opt_runs_per_step)\n",
        "global_var.set('weight_dir', weight_dir)\n",
        "global_var.set('log_dir', log_dir)\n",
        "global_var.set('output_dir', output_dir)\n",
        "global_var.set('d_arch', d_arch)\n",
        "global_var.set('g_arch', g_arch)\n",
        "global_var.set('epochs', epochs)\n",
        "\n",
        "# validation test for architecture string\n",
        "if d_arch not in ['selu', 'bn_first', 'bn_last']:\n",
        "    raise ValueError('d_arch should be one of \"selu\", \"bn_first\" or \"bn_last\"')\n",
        "if g_arch not in ['selu', 'bn_first', 'bn_last']:\n",
        "    raise ValueError('g_arch should be one of \"selu\", \"bn_first\" or \"bn_last\"')\n",
        "\n",
        "# loading the training set into train_x, updating the channel_count\n",
        "print('** LOADING TRAINING SET **')\n",
        "train_x = load_training_set()\n",
        "channel_count = train_x.shape[-1]\n",
        "global_var.set('channel_count', channel_count)\n",
        "print(train_x.shape, train_x.dtype)\n",
        "\n",
        "# constructing the graph\n",
        "print('** CONSTRUCTING VARIABLES AND COMPUTE GRAPH **')\n",
        "image_input = tf.placeholder(tf.float32, [None, image_height, image_width, channel_count],\n",
        "                             name='discriminator/input')\n",
        "noise_input = tf.placeholder(tf.float32, [None, noise_dim], name='generator/input')\n",
        "dropout_input = tf.placeholder(tf.float32, name='dropout_rate')\n",
        "summary_dict = dict()\n",
        "\n",
        "d, d_logits = discriminator_model(image_input, dropout_input)\n",
        "g = generator_model(noise_input, dropout_input)\n",
        "gan, gan_logits = discriminator_model(g, dropout_input, reuse=True)\n",
        "sampler = generator_model(noise_input, dropout_input, reuse=True, train=False)\n",
        "\n",
        "# generating the loss function\n",
        "d_loss, g_loss = compile_loss(d_logits, gan_logits, summary_dict)\n",
        "t_vars = tf.trainable_variables()\n",
        "d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
        "g_vars = [var for var in t_vars if 'generator' in var.name]\n",
        "d_opt = tf.train.AdamOptimizer(d_adam_lr, beta1=d_adam_beta1).minimize(d_loss, var_list=d_vars)\n",
        "g_opt = tf.train.AdamOptimizer(g_adam_lr, beta1=g_adam_beta1).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "tf.contrib.slim.model_analyzer.analyze_vars(t_vars, print_info=True)\n",
        "\n",
        "# set the sample noise using the specified seed\n",
        "np.random.seed(0)\n",
        "sample_noise = np.random.uniform(-1.0, 1.0, size=[sample_count, noise_dim])\n",
        "import time\n",
        "\n",
        "t = int(time.time())\n",
        "np.random.seed(t)\n",
        "\n",
        "summary_dict['d_pred'] = tf.summary.histogram('d_pred', tf.reshape(tf.concat([d, gan], axis=0), [1, -1]))\n",
        "summary_dict['g_pred'] = tf.summary.image('g_pred', g, max_outputs=random_count)\n",
        "summary_dict['g_trace'] = tf.summary.image('g_trace', g, max_outputs=sample_count)\n",
        "summary_dict['d_lr'] = tf.summary.scalar('d_lr', d_adam_lr)\n",
        "summary_dict['d_beta1'] = tf.summary.scalar('d_beta1', d_adam_beta1)\n",
        "summary_dict['g_lr'] = tf.summary.scalar('g_lr', g_adam_lr)\n",
        "summary_dict['g_beta1'] = tf.summary.scalar('g_beta1', g_adam_beta1)\n",
        "\n",
        "# starting the session\n",
        "print(\"** STARTING SESSION **\")\n",
        "saver = tf.train.Saver()\n",
        "sess = set_vram_growth()\n",
        "tf.global_variables_initializer().run(session=sess)\n",
        "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
        "\n",
        "# creating new directory\n",
        "if not os.path.exists(log_dir):\n",
        "    os.mkdir(log_dir)\n",
        "if not os.path.exists(weight_dir):\n",
        "    os.mkdir(weight_dir)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** LOADING TRAINING SET **\n",
            "(5619, 100, 100, 3) float32\n",
            "** CONSTRUCTING VARIABLES AND COMPUTE GRAPH **\n",
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "discriminator/layer1/conv2d/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
            "discriminator/layer1/conv2d/b:0 (float32_ref 64) [64, bytes: 256]\n",
            "discriminator/layer2/conv2d/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
            "discriminator/layer2/conv2d/b:0 (float32_ref 128) [128, bytes: 512]\n",
            "discriminator/layer2/bn/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "discriminator/layer2/bn/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "discriminator/layer3/conv2d/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
            "discriminator/layer3/conv2d/b:0 (float32_ref 256) [256, bytes: 1024]\n",
            "discriminator/layer3/bn/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "discriminator/layer3/bn/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "discriminator/layer4/conv2d/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]\n",
            "discriminator/layer4/conv2d/b:0 (float32_ref 512) [512, bytes: 2048]\n",
            "discriminator/layer4/bn/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "discriminator/layer4/bn/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "discriminator/layer5/dense/w:0 (float32_ref 25088x1) [25088, bytes: 100352]\n",
            "discriminator/layer5/dense/b:0 (float32_ref 1) [1, bytes: 4]\n",
            "generator/layer0/fc/w:0 (float32_ref 100x25088) [2508800, bytes: 10035200]\n",
            "generator/layer0/fc/b:0 (float32_ref 25088) [25088, bytes: 100352]\n",
            "generator/layer0/bn/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "generator/layer0/bn/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "generator/layer1/deconv2d/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]\n",
            "generator/layer1/deconv2d/b:0 (float32_ref 256) [256, bytes: 1024]\n",
            "generator/layer1/bn/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "generator/layer1/bn/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "generator/layer2/deconv2d/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
            "generator/layer2/deconv2d/b:0 (float32_ref 128) [128, bytes: 512]\n",
            "generator/layer2/bn/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "generator/layer2/bn/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "generator/layer3/deconv2d/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
            "generator/layer3/deconv2d/b:0 (float32_ref 64) [64, bytes: 256]\n",
            "generator/layer3/bn/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "generator/layer3/bn/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "generator/layer4/deconv2d/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
            "generator/layer4/deconv2d/b:0 (float32_ref 3) [3, bytes: 12]\n",
            "Total size of variables: 11175300\n",
            "Total bytes of variables: 44701200\n",
            "** STARTING SESSION **\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G1YNUBw7Hzj5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. In-Training Process\n",
        "Here is the code for loading existed model, training model and save current model\n",
        "\n",
        "You can run this cell repeatly whatever you want"
      ]
    },
    {
      "metadata": {
        "id": "m6lmS7Y06xP7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            },
            {
              "item_id": 46
            },
            {
              "item_id": 47
            },
            {
              "item_id": 92
            },
            {
              "item_id": 93
            },
            {
              "item_id": 139
            },
            {
              "item_id": 140
            },
            {
              "item_id": 185
            },
            {
              "item_id": 186
            },
            {
              "item_id": 231
            },
            {
              "item_id": 232
            },
            {
              "item_id": 277
            },
            {
              "item_id": 278
            },
            {
              "item_id": 323
            },
            {
              "item_id": 324
            },
            {
              "item_id": 369
            },
            {
              "item_id": 370
            },
            {
              "item_id": 415
            },
            {
              "item_id": 416
            },
            {
              "item_id": 461
            },
            {
              "item_id": 462
            },
            {
              "item_id": 507
            },
            {
              "item_id": 508
            },
            {
              "item_id": 524
            },
            {
              "item_id": 525
            },
            {
              "item_id": 554
            },
            {
              "item_id": 555
            },
            {
              "item_id": 600
            },
            {
              "item_id": 601
            },
            {
              "item_id": 646
            },
            {
              "item_id": 647
            },
            {
              "item_id": 691
            },
            {
              "item_id": 692
            },
            {
              "item_id": 737
            },
            {
              "item_id": 738
            },
            {
              "item_id": 783
            },
            {
              "item_id": 784
            },
            {
              "item_id": 829
            },
            {
              "item_id": 830
            },
            {
              "item_id": 875
            },
            {
              "item_id": 876
            },
            {
              "item_id": 921
            },
            {
              "item_id": 922
            },
            {
              "item_id": 967
            },
            {
              "item_id": 968
            },
            {
              "item_id": 1013
            },
            {
              "item_id": 1014
            },
            {
              "item_id": 1046
            },
            {
              "item_id": 1047
            },
            {
              "item_id": 1060
            },
            {
              "item_id": 1061
            },
            {
              "item_id": 1105
            },
            {
              "item_id": 1106
            },
            {
              "item_id": 1151
            },
            {
              "item_id": 1152
            },
            {
              "item_id": 1197
            },
            {
              "item_id": 1198
            },
            {
              "item_id": 1242
            },
            {
              "item_id": 1243
            },
            {
              "item_id": 1288
            },
            {
              "item_id": 1289
            },
            {
              "item_id": 1334
            },
            {
              "item_id": 1335
            },
            {
              "item_id": 1380
            },
            {
              "item_id": 1381
            },
            {
              "item_id": 1426
            },
            {
              "item_id": 1427
            },
            {
              "item_id": 1472
            },
            {
              "item_id": 1473
            },
            {
              "item_id": 1518
            },
            {
              "item_id": 1519
            },
            {
              "item_id": 1564
            },
            {
              "item_id": 1565
            },
            {
              "item_id": 1569
            },
            {
              "item_id": 1570
            },
            {
              "item_id": 1608
            },
            {
              "item_id": 1609
            },
            {
              "item_id": 1653
            },
            {
              "item_id": 1654
            },
            {
              "item_id": 1698
            },
            {
              "item_id": 1699
            },
            {
              "item_id": 1744
            },
            {
              "item_id": 1745
            },
            {
              "item_id": 1789
            },
            {
              "item_id": 1790
            },
            {
              "item_id": 1833
            },
            {
              "item_id": 1834
            },
            {
              "item_id": 1879
            },
            {
              "item_id": 1880
            },
            {
              "item_id": 1925
            },
            {
              "item_id": 1926
            },
            {
              "item_id": 1971
            },
            {
              "item_id": 1972
            },
            {
              "item_id": 2017
            },
            {
              "item_id": 2018
            },
            {
              "item_id": 2062
            },
            {
              "item_id": 2063
            },
            {
              "item_id": 2083
            },
            {
              "item_id": 2084
            },
            {
              "item_id": 2109
            },
            {
              "item_id": 2110
            },
            {
              "item_id": 2155
            },
            {
              "item_id": 2156
            },
            {
              "item_id": 2201
            },
            {
              "item_id": 2202
            },
            {
              "item_id": 2247
            },
            {
              "item_id": 2248
            },
            {
              "item_id": 2293
            },
            {
              "item_id": 2294
            },
            {
              "item_id": 2339
            },
            {
              "item_id": 2340
            },
            {
              "item_id": 2385
            },
            {
              "item_id": 2386
            },
            {
              "item_id": 2427
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 2941
        },
        "outputId": "ab64915c-a067-4881-dd88-5bf0d7dd24c7"
      },
      "cell_type": "code",
      "source": [
        "saver = load_model(sess, saver, weight_dir)\n",
        "\n",
        "# start training process\n",
        "print('** TRAINING PROCESS STARTED **')\n",
        "if epochs <= 0:\n",
        "    print('Press \"Q\" to exit this program')\n",
        "\n",
        "train_model(sess, saver, train_x, d_opt, g_opt, sampler, image_input, noise_input, sample_noise, dropout_input,\n",
        "            summary_dict, summary_writer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model loaded, restore step 0\n",
            "** TRAINING PROCESS STARTED **\n",
            "\n",
            "[Epoch 1 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:07<00:00,  1.54s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 2 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 3 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|##5       | 11/44 [00:16<00:49,  1.50s/it]/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n",
            "100%|##########| 44/44 [01:06<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 4 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 5 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 6 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 7 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 8 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 9 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 10 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 11 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 12 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 36%|###6      | 16/44 [00:25<00:44,  1.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model saved, save step 500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 13 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 14 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 15 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 16 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 17 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 18 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 19 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 20 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 21 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 22 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 23 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 73%|#######2  | 32/44 [00:49<00:18,  1.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model saved, save step 1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 24 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 25 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 26 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 27 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 28 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 29 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 30 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 31 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 32 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 33 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 34 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 35 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|9         | 4/44 [00:07<01:11,  1.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model saved, save step 1500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 36 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 37 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 38 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 39 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 40 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 41 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 42 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 43 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 44 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 45 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 46 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|####5     | 20/44 [00:31<00:37,  1.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model saved, save step 2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:07<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 47 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 48 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.51s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 49 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.49s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 50 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:06<00:00,  1.52s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 51 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 52 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|##########| 44/44 [01:05<00:00,  1.50s/it]\n",
            "  0%|          | 0/44 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Epoch 53 of 100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 93%|#########3| 41/44 [01:02<00:04,  1.52s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "aiLwyc4pIJQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.Post-Training Process\n",
        "Closing the session and file handles, releasing all used resources"
      ]
    },
    {
      "metadata": {
        "id": "-_o3-DiB4_F8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3b5ef24-5eb6-4fed-f9fd-34338af80e33",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521532872344,
          "user_tz": -480,
          "elapsed": 990,
          "user": {
            "displayName": "qhgz2011@hotmail.com",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111469927240443201404"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess.close()\n",
        "summary_writer.close()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J_uoIsJbIWfA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. Sync the training data to your Google Drive\n",
        "call `sync_result()` to sync the data to specified directory (default `tf_colab`)"
      ]
    },
    {
      "metadata": {
        "id": "QhGsH5Di7YMN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sync_result(target_dir='tf_colab'):\n",
        "  #entering root directory\n",
        "  files = drive.ListFile({'q': \"trashed=false and 'root' in parents\"}).GetList()\n",
        "  root_file = None\n",
        "  weight_dir = global_var.get('weight_dir')\n",
        "  log_dir = global_var.get('log_dir')\n",
        "  output_dir = global_var.get('output_dir')\n",
        "  for file in files:\n",
        "    if file['title'] == target_dir:\n",
        "      root_file = file\n",
        "      break\n",
        "  if root_file == None:\n",
        "    raise ValueError('ID for target_dir not found')\n",
        "  #deleting existed directories\n",
        "  files = drive.ListFile({'q': \"trashed=false and '%s' in parents\" % root_file['id']}).GetList()\n",
        "  for file in files:\n",
        "    deleted = drive.CreateFile({'id': file['id']})\n",
        "    deleted.Delete()\n",
        "  #creating new directories\n",
        "  def create_dir(dir_name):\n",
        "    meta = {'title': dir_name, 'mimeType': 'application/vnd.google-apps.folder',\n",
        "            'parents': [{'id': root_file['id']}]}\n",
        "    dir_info = drive.CreateFile(meta)\n",
        "    dir_info.Upload()\n",
        "    return dir_info\n",
        "  weight_dir_info = create_dir(weight_dir)\n",
        "  log_dir_info = create_dir(log_dir)\n",
        "  output_dir_info = create_dir(output_dir)\n",
        "  #listing the local files\n",
        "  def upload_dir(dir_name, parent_id):\n",
        "    files = os.listdir(dir_name)\n",
        "    for file in files:\n",
        "      uploaded = drive.CreateFile({'title': file, 'parents': [{'id': parent_id}]})\n",
        "      uploaded.SetContentFile(os.path.join(dir_name, file))\n",
        "      uploaded.Upload()\n",
        "  upload_dir(weight_dir, weight_dir_info['id'])\n",
        "  upload_dir(log_dir, log_dir_info['id'])\n",
        "  upload_dir(output_dir, output_dir_info['id'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZ6Y-lq-7azp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sync_result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qkZ4FuH31c6c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}